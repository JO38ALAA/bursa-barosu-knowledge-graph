"""
Bursa Barosu Bilgi GrafÄ± Projesi - Ana Orchestrator Script
TÃ¼m sÃ¼reci yÃ¶neten ana script
"""
import os
import json
import logging
from datetime import datetime
from tqdm import tqdm
import sys

# Proje modÃ¼llerini import et
from crawler.scraper import BursaBaroScraper
from nlp.processor import NLPProcessor
from graph.builder import GraphBuilder
from config import RAW_DATA_DIR, PROCESSED_DATA_DIR

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BursaBaroKnowledgeGraph:
    """Ana orchestrator sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        """Orchestrator'Ä± baÅŸlat"""
        self.scraper = None
        self.nlp_processor = None
        self.graph_builder = None
        
        # Veri dizinlerini oluÅŸtur
        os.makedirs(RAW_DATA_DIR, exist_ok=True)
        os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
        
        logger.info("Bursa Barosu Bilgi GrafÄ± Projesi baÅŸlatÄ±ldÄ±")
    
    def initialize_components(self):
        """TÃ¼m bileÅŸenleri baÅŸlat"""
        try:
            logger.info("BileÅŸenler baÅŸlatÄ±lÄ±yor...")
            
            self.scraper = BursaBaroScraper()
            self.nlp_processor = NLPProcessor()
            self.graph_builder = GraphBuilder()
            
            logger.info("TÃ¼m bileÅŸenler baÅŸarÄ±yla baÅŸlatÄ±ldÄ±")
            return True
            
        except Exception as e:
            logger.error(f"BileÅŸen baÅŸlatma hatasÄ±: {e}")
            return False
    
    def step1_fetch_urls(self) -> list:
        """AdÄ±m 1: Sitemap'ten URL'leri Ã§ek"""
        logger.info("=== ADIM 1: URL'ler Ã§ekiliyor ===")
        
        urls = self.scraper.fetch_sitemap()
        
        if not urls:
            logger.warning("HiÃ§ URL bulunamadÄ±, ana sayfa ile devam ediliyor")
            urls = ["https://bursabarosu.org.tr"]
        
        logger.info(f"Toplam {len(urls)} URL bulundu")
        return urls
    
    def step2_scrape_pages(self, urls: list) -> int:
        """AdÄ±m 2: SayfalarÄ± kazÄ± ve raw data olarak kaydet"""
        logger.info("=== ADIM 2: Sayfalar kazÄ±nÄ±yor ===")
        
        scraped_count = 0
        failed_count = 0
        
        # Progress bar ile scraping
        for i, url in enumerate(tqdm(urls, desc="Sayfalar kazÄ±nÄ±yor")):
            try:
                # Sayfa verilerini Ã§ek
                page_data = self.scraper.scrape_page(url)
                
                if page_data:
                    # Dosya adÄ±nÄ± oluÅŸtur (URL'den gÃ¼venli dosya adÄ±)
                    safe_filename = self._url_to_filename(url)
                    file_path = os.path.join(RAW_DATA_DIR, f"{safe_filename}.json")
                    
                    # Metadata ekle
                    page_data['scraped_at'] = datetime.now().isoformat()
                    page_data['scrape_index'] = i
                    
                    # JSON olarak kaydet
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(page_data, f, ensure_ascii=False, indent=2)
                    
                    scraped_count += 1
                    logger.debug(f"Kaydedildi: {file_path}")
                else:
                    failed_count += 1
                    logger.warning(f"Sayfa Ã§ekilemedi: {url}")
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"Scraping hatasÄ± ({url}): {e}")
        
        logger.info(f"Scraping tamamlandÄ±: {scraped_count} baÅŸarÄ±lÄ±, {failed_count} baÅŸarÄ±sÄ±z")
        return scraped_count
    
    def step3_process_nlp(self) -> int:
        """AdÄ±m 3: Raw verileri NLP ile iÅŸle"""
        logger.info("=== ADIM 3: NLP iÅŸleme baÅŸlÄ±yor ===")
        
        # Raw data dosyalarÄ±nÄ± bul
        raw_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.json')]
        
        if not raw_files:
            logger.error("Ä°ÅŸlenecek raw data dosyasÄ± bulunamadÄ±")
            return 0
        
        processed_count = 0
        
        # Progress bar ile NLP processing
        for filename in tqdm(raw_files, desc="NLP iÅŸleme"):
            try:
                raw_file_path = os.path.join(RAW_DATA_DIR, filename)
                processed_file_path = os.path.join(PROCESSED_DATA_DIR, filename)
                
                # Raw veriyi oku
                with open(raw_file_path, 'r', encoding='utf-8') as f:
                    raw_data = json.load(f)
                
                # NLP iÅŸleme
                nlp_result = self.nlp_processor.process_text(raw_data['content'])
                
                # Ä°ÅŸlenmiÅŸ veriyi hazÄ±rla
                processed_data = {
                    'url': raw_data['url'],
                    'title': raw_data['title'],
                    'content': raw_data['content'],
                    'scraped_at': raw_data.get('scraped_at'),
                    'processed_at': datetime.now().isoformat(),
                    'nlp_result': nlp_result
                }
                
                # Ä°ÅŸlenmiÅŸ veriyi kaydet
                with open(processed_file_path, 'w', encoding='utf-8') as f:
                    json.dump(processed_data, f, ensure_ascii=False, indent=2)
                
                processed_count += 1
                logger.debug(f"NLP iÅŸlendi: {filename}")
                
            except Exception as e:
                logger.error(f"NLP iÅŸleme hatasÄ± ({filename}): {e}")
        
        logger.info(f"NLP iÅŸleme tamamlandÄ±: {processed_count} dosya iÅŸlendi")
        return processed_count
    
    def step4_build_graph(self) -> int:
        """AdÄ±m 4: Ä°ÅŸlenmiÅŸ verileri graf veritabanÄ±na yaz"""
        logger.info("=== ADIM 4: Graf veritabanÄ± oluÅŸturuluyor ===")
        
        # Ä°ÅŸlenmiÅŸ data dosyalarÄ±nÄ± bul
        processed_files = [f for f in os.listdir(PROCESSED_DATA_DIR) if f.endswith('.json')]
        
        if not processed_files:
            logger.error("Ä°ÅŸlenecek processed data dosyasÄ± bulunamadÄ±")
            return 0
        
        graph_count = 0
        
        # Progress bar ile graph building
        for filename in tqdm(processed_files, desc="Graf oluÅŸturuluyor"):
            try:
                processed_file_path = os.path.join(PROCESSED_DATA_DIR, filename)
                
                # Ä°ÅŸlenmiÅŸ veriyi oku
                with open(processed_file_path, 'r', encoding='utf-8') as f:
                    processed_data = json.load(f)
                
                # DokÃ¼man dÃ¼ÄŸÃ¼mÃ¼nÃ¼ oluÅŸtur
                document_data = {
                    'url': processed_data['url'],
                    'title': processed_data['title'],
                    'content': processed_data['content']
                }
                
                success = self.graph_builder.create_document_node(document_data)
                if not success:
                    logger.warning(f"DokÃ¼man dÃ¼ÄŸÃ¼mÃ¼ oluÅŸturulamadÄ±: {filename}")
                    continue
                
                # VarlÄ±k dÃ¼ÄŸÃ¼mlerini oluÅŸtur
                nlp_result = processed_data['nlp_result']
                entities = nlp_result.get('entities', [])
                relationships = nlp_result.get('relationships', [])
                
                # VarlÄ±klarÄ± oluÅŸtur
                for entity in entities:
                    self.graph_builder.create_or_update_node(entity)
                
                # VarlÄ±klarÄ± dokÃ¼mana baÄŸla
                if entities:
                    self.graph_builder.link_entities_to_document(entities, processed_data['url'])
                
                # Ä°liÅŸkileri oluÅŸtur
                for relationship in relationships:
                    entity1 = {
                        'text': relationship['entity1'],
                        'label': relationship['entity1_label']
                    }
                    entity2 = {
                        'text': relationship['entity2'], 
                        'label': relationship['entity2_label']
                    }
                    
                    self.graph_builder.create_relationship(
                        entity1, 
                        entity2, 
                        relationship['relation_type'],
                        processed_data['url']
                    )
                
                graph_count += 1
                logger.debug(f"Graf'a eklendi: {filename}")
                
            except Exception as e:
                logger.error(f"Graf oluÅŸturma hatasÄ± ({filename}): {e}")
        
        logger.info(f"Graf oluÅŸturma tamamlandÄ±: {graph_count} dosya iÅŸlendi")
        return graph_count
    
    def run_full_pipeline(self, max_pages=None):
        """Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r"""
        logger.info("ğŸš€ TAM PÄ°PELÄ°NE BAÅLATIYOR ğŸš€")
        
        # BileÅŸenleri baÅŸlat
        if not self.initialize_components():
            logger.error("BileÅŸenler baÅŸlatÄ±lamadÄ±, pipeline durduruluyor")
            return False
        
        try:
            # AdÄ±m 1: URL'leri Ã§ek
            urls = self.step1_fetch_urls()
            if not urls:
                logger.error("URL'ler Ã§ekilemedi")
                return False
            
            # Max pages limiti uygula
            if max_pages and max_pages < len(urls):
                logger.info(f"URL listesi {len(urls)}'den {max_pages}'e sÄ±nÄ±rlandÄ±rÄ±ldÄ±")
                urls = urls[:max_pages]
            
            # AdÄ±m 2: SayfalarÄ± kazÄ±
            scraped_count = self.step2_scrape_pages(urls)
            if scraped_count == 0:
                logger.error("HiÃ§ sayfa kazÄ±namadÄ±")
                return False
            
            # AdÄ±m 3: NLP iÅŸleme
            processed_count = self.step3_process_nlp()
            if processed_count == 0:
                logger.error("HiÃ§ veri iÅŸlenemedi")
                return False
            
            # AdÄ±m 4: Graf oluÅŸtur
            graph_count = self.step4_build_graph()
            if graph_count == 0:
                logger.error("Graf oluÅŸturulamadÄ±")
                return False
            
            # Final istatistikler
            stats = self.graph_builder.get_graph_stats()
            
            logger.info("ğŸ‰ PÄ°PELÄ°NE BAÅARIYLA TAMAMLANDI! ğŸ‰")
            logger.info(f"ğŸ“Š Ã–zet Ä°statistikler:")
            logger.info(f"   â€¢ Bulunan URL: {len(urls)}")
            logger.info(f"   â€¢ KazÄ±nan sayfa: {scraped_count}")
            logger.info(f"   â€¢ Ä°ÅŸlenen dosya: {processed_count}")
            logger.info(f"   â€¢ Graf'a eklenen: {graph_count}")
            logger.info(f"   â€¢ Toplam dÃ¼ÄŸÃ¼m: {stats.get('total_nodes', 0)}")
            logger.info(f"   â€¢ Toplam iliÅŸki: {stats.get('relationships', 0)}")
            
            return True
            
        except Exception as e:
            logger.error(f"Pipeline hatasÄ±: {e}")
            return False
        
        finally:
            self.cleanup()
    
    def _url_to_filename(self, url: str) -> str:
        """URL'yi gÃ¼venli dosya adÄ±na Ã§evir"""
        import re
        # URL'den protokol ve Ã¶zel karakterleri temizle
        filename = re.sub(r'https?://', '', url)
        filename = re.sub(r'[^\w\-_.]', '_', filename)
        filename = filename[:100]  # Maksimum uzunluk sÄ±nÄ±rÄ±
        return filename
    
    def cleanup(self):
        """KaynaklarÄ± temizle"""
        try:
            if self.scraper:
                self.scraper.close()
            if self.graph_builder:
                self.graph_builder.close()
            logger.info("Kaynaklar temizlendi")
        except Exception as e:
            logger.error(f"Temizleme hatasÄ±: {e}")


def main():
    """Ana fonksiyon"""
    import argparse
    
    # Komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± parse et
    parser = argparse.ArgumentParser(description='Bursa Barosu Bilgi GrafÄ± Pipeline')
    parser.add_argument('--max-pages', type=int, default=None, 
                       help='Maksimum iÅŸlenecek sayfa sayÄ±sÄ± (varsayÄ±lan: tÃ¼mÃ¼)')
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ›ï¸  BURSA BAROSU BÄ°LGÄ° GRAFI PROJESÄ°  ğŸ›ï¸")
    print("=" * 60)
    
    if args.max_pages:
        print(f"ğŸ“„ Maksimum sayfa limiti: {args.max_pages}")
    else:
        print("ğŸ“„ TÃ¼m sayfalar iÅŸlenecek")
    
    # Orchestrator'Ä± baÅŸlat
    orchestrator = BursaBaroKnowledgeGraph()
    
    # Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
    success = orchestrator.run_full_pipeline(max_pages=args.max_pages)
    
    if success:
        print("\nâœ… Proje baÅŸarÄ±yla tamamlandÄ±!")
        print("ğŸŒ Neo4j Browser'da sonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼leyebilirsiniz: http://localhost:7474")
        print("ğŸ“Š Graf sorgularÄ±nÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz:")
        print("   MATCH (n) RETURN n LIMIT 25")
        print("   MATCH (p:Person)-[r]->(o:Organization) RETURN p, r, o")
    else:
        print("\nâŒ Proje tamamlanamadÄ±. Loglara bakÄ±nÄ±z.")
    
    print("=" * 60)


if __name__ == "__main__":
    main()
