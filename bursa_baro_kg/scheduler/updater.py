"""
Bursa Barosu Otomatik Veri GÃ¼ncelleme Sistemi
Belirli aralÄ±klarla web sitesini tarayÄ±p graf'Ä± gÃ¼nceller
"""
import schedule
import time
import logging
import threading
from datetime import datetime, timedelta
import sys
import os
from typing import Dict, Any

# Proje modÃ¼llerini import et
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from crawler.scraper import BursaBaroScraper
from nlp.processor import NLPProcessor
from graph.builder import GraphBuilder
from cache.manager import CacheManager

logger = logging.getLogger(__name__)


class AutoUpdater:
    """Otomatik veri gÃ¼ncelleme sistemi"""
    
    def __init__(self, 
                 update_interval_hours: int = 24,
                 max_pages_per_update: int = 200):
        """
        Auto Updater'Ä± baÅŸlat
        
        Args:
            update_interval_hours: GÃ¼ncelleme aralÄ±ÄŸÄ± (saat)
            max_pages_per_update: Her gÃ¼ncellemede maksimum sayfa sayÄ±sÄ±
        """
        self.update_interval = update_interval_hours
        self.max_pages = max_pages_per_update
        self.is_running = False
        self.last_update = None
        self.update_stats = {
            'total_updates': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'last_update_time': None,
            'last_update_duration': None,
            'pages_processed': 0,
            'entities_added': 0,
            'relationships_added': 0
        }
        
        # Cache manager (gÃ¼ncellemeler sonrasÄ± cache temizlemek iÃ§in)
        self.cache = CacheManager()
        
        logger.info(f"Auto Updater baÅŸlatÄ±ldÄ± - GÃ¼ncelleme aralÄ±ÄŸÄ±: {update_interval_hours} saat")
    
    def start_scheduler(self):
        """Scheduler'Ä± baÅŸlat"""
        if self.is_running:
            logger.warning("Scheduler zaten Ã§alÄ±ÅŸÄ±yor")
            return
        
        # GÃ¼nlÃ¼k gÃ¼ncelleme planla
        schedule.every(self.update_interval).hours.do(self.run_update)
        
        # Ä°lk gÃ¼ncellemeyi hemen yap (opsiyonel)
        # schedule.every().minute.do(self.run_update).tag('immediate')
        
        self.is_running = True
        logger.info(f"Scheduler baÅŸlatÄ±ldÄ± - {self.update_interval} saatte bir gÃ¼ncelleme yapÄ±lacak")
        
        # Scheduler thread'i baÅŸlat
        self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.scheduler_thread.start()
    
    def stop_scheduler(self):
        """Scheduler'Ä± durdur"""
        self.is_running = False
        schedule.clear()
        logger.info("Scheduler durduruldu")
    
    def _run_scheduler(self):
        """Scheduler'Ä± Ã§alÄ±ÅŸtÄ±r (thread iÃ§inde)"""
        while self.is_running:
            schedule.run_pending()
            time.sleep(60)  # Her dakika kontrol et
    
    def run_update(self):
        """Manuel gÃ¼ncelleme Ã§alÄ±ÅŸtÄ±r"""
        if not self.is_running:
            logger.warning("Scheduler Ã§alÄ±ÅŸmÄ±yor, gÃ¼ncelleme atlandÄ±")
            return
        
        start_time = datetime.now()
        logger.info("ğŸ”„ Otomatik veri gÃ¼ncelleme baÅŸlatÄ±ldÄ±")
        
        try:
            self.update_stats['total_updates'] += 1
            
            # 1. Web sitesini tara
            scraper = BursaBaroScraper()
            urls = scraper.fetch_sitemap()
            
            if not urls:
                logger.warning("URL bulunamadÄ±, gÃ¼ncelleme atlandÄ±")
                self.update_stats['failed_updates'] += 1
                return
            
            # TÃ¼m sayfalarÄ± iÅŸle
            urls = urls[:self.max_pages]
            
            logger.info(f"ğŸ“„ {len(urls)} sayfa iÅŸlenecek")
            
            # 2. SayfalarÄ± kazÄ±
            scraped_data = []
            for i, url in enumerate(urls):
                try:
                    page_data = scraper.scrape_page(url)
                    if page_data and page_data.get('content') and len(page_data['content'].strip()) > 100:
                        scraped_data.append({
                            'url': url,
                            'title': page_data.get('title', ''),
                            'content': page_data['content'],
                            'scraped_at': datetime.now().isoformat()
                        })
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"ğŸ“„ {i + 1}/{len(urls)} sayfa kazÄ±ndÄ±")
                        
                except Exception as e:
                    logger.error(f"Sayfa kazÄ±ma hatasÄ± {url}: {e}")
                    continue
            
            scraper.close()
            
            if not scraped_data:
                logger.warning("HiÃ§ veri kazÄ±namadÄ±, gÃ¼ncelleme atlandÄ±")
                self.update_stats['failed_updates'] += 1
                return
            
            logger.info(f"âœ… {len(scraped_data)} sayfa baÅŸarÄ±yla kazÄ±ndÄ±")
            
            # 3. NLP iÅŸleme
            nlp_processor = NLPProcessor()
            processed_data = []
            
            total_entities = 0
            total_relationships = 0
            
            for data in scraped_data:
                try:
                    result = nlp_processor.process_text(data['content'])
                    
                    processed_data.append({
                        'url': data['url'],
                        'content': data['content'],
                        'entities': result['entities'],
                        'relationships': result['relationships'],
                        'scraped_at': data['scraped_at']
                    })
                    
                    total_entities += len(result['entities'])
                    total_relationships += len(result['relationships'])
                    
                except Exception as e:
                    logger.error(f"NLP iÅŸleme hatasÄ± {data['url']}: {e}")
                    continue
            
            logger.info(f"ğŸ§  NLP iÅŸleme tamamlandÄ±: {total_entities} varlÄ±k, {total_relationships} iliÅŸki")
            
            # 4. Graf'a ekle
            graph_builder = GraphBuilder()
            
            for data in processed_data:
                try:
                    # DokÃ¼man dÃ¼ÄŸÃ¼mÃ¼ oluÅŸtur
                    document_data = {
                        'url': data['url'],
                        'title': data.get('title', ''),
                        'content': data['content']
                    }
                    graph_builder.create_document_node(document_data)
                    
                    # VarlÄ±klarÄ± ekle
                    for entity in data['entities']:
                        graph_builder.create_entity_node(
                            entity['text'], 
                            entity['label']
                        )
                        # VarlÄ±ÄŸÄ± dokÃ¼manla baÄŸla
                        entity_dict = {
                            'text': entity['text'],
                            'label': entity['label']
                        }
                        document_dict = {
                            'text': data['url'],
                            'label': 'Document'
                        }
                        graph_builder.create_relationship(
                            entity_dict,
                            document_dict,
                            'MENTIONED_IN',
                            data['url']
                        )
                    
                    # Ä°liÅŸkileri ekle
                    for rel in data['relationships']:
                        entity1 = {
                            'text': rel['entity1'],
                            'label': rel['entity1_label']
                        }
                        entity2 = {
                            'text': rel['entity2'], 
                            'label': rel['entity2_label']
                        }
                        graph_builder.create_relationship(
                            entity1,
                            entity2,
                            rel['relation_type'],
                            data['url']
                        )
                
                except Exception as e:
                    logger.error(f"Graf gÃ¼ncelleme hatasÄ± {data['url']}: {e}")
                    continue
            
            graph_builder.close()
            
            # 5. Cache'i temizle (yeni veriler iÃ§in)
            self.cache.clear()
            logger.info("ğŸ—‘ï¸ Cache temizlendi")
            
            # Ä°statistikleri gÃ¼ncelle
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.update_stats.update({
                'successful_updates': self.update_stats['successful_updates'] + 1,
                'last_update_time': end_time.isoformat(),
                'last_update_duration': duration,
                'pages_processed': len(scraped_data),
                'entities_added': total_entities,
                'relationships_added': total_relationships
            })
            
            self.last_update = end_time
            
            logger.info(f"âœ… Otomatik gÃ¼ncelleme tamamlandÄ± - SÃ¼re: {duration:.1f}s")
            logger.info(f"ğŸ“Š Ä°ÅŸlenen: {len(scraped_data)} sayfa, {total_entities} varlÄ±k, {total_relationships} iliÅŸki")
            
        except Exception as e:
            logger.error(f"âŒ Otomatik gÃ¼ncelleme hatasÄ±: {e}")
            self.update_stats['failed_updates'] += 1
    
    def get_status(self) -> Dict[str, Any]:
        """GÃ¼ncelleyici durumunu getir"""
        next_run = None
        if self.is_running and schedule.jobs:
            next_job = min(schedule.jobs, key=lambda job: job.next_run)
            next_run = next_job.next_run.isoformat() if next_job.next_run else None
        
        return {
            'is_running': self.is_running,
            'update_interval_hours': self.update_interval,
            'max_pages_per_update': self.max_pages,
            'next_scheduled_update': next_run,
            'last_update': self.last_update.isoformat() if self.last_update else None,
            'stats': self.update_stats
        }
    
    def force_update(self):
        """Zorla gÃ¼ncelleme yap (manuel tetikleme)"""
        logger.info("ğŸ”„ Manuel gÃ¼ncelleme tetiklendi")
        
        # Yeni thread'de Ã§alÄ±ÅŸtÄ±r (blocking olmamak iÃ§in)
        update_thread = threading.Thread(target=self.run_update, daemon=True)
        update_thread.start()
        
        return {"message": "GÃ¼ncelleme baÅŸlatÄ±ldÄ±", "status": "started"}


# Global updater instance
_global_updater = None

def get_updater() -> AutoUpdater:
    """Global updater instance'Ä±nÄ± al"""
    global _global_updater
    if _global_updater is None:
        _global_updater = AutoUpdater(update_interval_hours=24, max_pages_per_update=20)
    return _global_updater


# Test ve CLI kullanÄ±mÄ±
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Bursa Barosu Otomatik GÃ¼ncelleme Sistemi")
    parser.add_argument("--start", action="store_true", help="Scheduler'Ä± baÅŸlat")
    parser.add_argument("--update", action="store_true", help="Tek seferlik gÃ¼ncelleme yap")
    parser.add_argument("--status", action="store_true", help="Durum bilgisi gÃ¶ster")
    parser.add_argument("--interval", type=int, default=24, help="GÃ¼ncelleme aralÄ±ÄŸÄ± (saat)")
    parser.add_argument("--max-pages", type=int, default=20, help="Maksimum sayfa sayÄ±sÄ±")
    
    args = parser.parse_args()
    
    # Logging ayarla
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    updater = AutoUpdater(
        update_interval_hours=args.interval,
        max_pages_per_update=args.max_pages
    )
    
    if args.start:
        print("ğŸš€ Scheduler baÅŸlatÄ±lÄ±yor...")
        updater.start_scheduler()
        
        try:
            # Ana thread'i canlÄ± tut
            while True:
                time.sleep(10)
                if not updater.is_running:
                    break
        except KeyboardInterrupt:
            print("\nâ¹ï¸ Scheduler durduruluyor...")
            updater.stop_scheduler()
    
    elif args.update:
        print("ğŸ”„ Tek seferlik gÃ¼ncelleme baÅŸlatÄ±lÄ±yor...")
        updater.is_running = True  # Manuel gÃ¼ncelleme iÃ§in
        updater.run_update()
    
    elif args.status:
        status = updater.get_status()
        print("ğŸ“Š Updater Durumu:")
        for key, value in status.items():
            print(f"   â€¢ {key}: {value}")
    
    else:
        parser.print_help()
